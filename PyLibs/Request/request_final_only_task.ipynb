{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-l-E-v/ML-Engineer/blob/main/request_final_only_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072ab693-f300-4712-9574-4dc7e21d78e2",
      "metadata": {
        "id": "072ab693-f300-4712-9574-4dc7e21d78e2"
      },
      "source": [
        "# Домашнее задание\n",
        "# Тема 2.2. Request"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yty8uM2qsao-",
      "metadata": {
        "id": "yty8uM2qsao-"
      },
      "source": [
        "Цель: научиться парсить данные при помощи библиотеки Request."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b84bce-a63d-4794-88d0-8718398519ef",
      "metadata": {
        "id": "11b84bce-a63d-4794-88d0-8718398519ef"
      },
      "source": [
        "# 1. Просто get-запрос"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e114783-db81-4929-b047-f8828f8405ee",
      "metadata": {
        "id": "0e114783-db81-4929-b047-f8828f8405ee",
        "tags": []
      },
      "source": [
        "### Задание\n",
        "С помощью библиотеки Requests отправьте get запрос на страницу https://ru.wikipedia.org/wiki/Капибара и выведите статус-код операции."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ca83aad1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Статус-код: 200\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Отправка GET-запроса на страницу Википедии о капибаре\n",
        "response = requests.get('https://ru.wikipedia.org/wiki/Капибара')\n",
        "\n",
        "# Вывод статус-кода операции\n",
        "print(f\"Статус-код: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aaf5d61-ef5b-451d-acfd-4ab19a021b4c",
      "metadata": {
        "id": "1aaf5d61-ef5b-451d-acfd-4ab19a021b4c",
        "tags": []
      },
      "source": [
        "## 2. Парсинг википедии"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30a13622-5510-4493-a6a8-3eda8ebd2c8e",
      "metadata": {
        "id": "30a13622-5510-4493-a6a8-3eda8ebd2c8e",
        "tags": []
      },
      "source": [
        "### Задание\n",
        "С помощью библиотеки Requests распарсите страницу https://ru.wikipedia.org/wiki/Биткойн, соберите ссылки на изображения (подсказка: атрибут src) и подписи к этим изображениям (подсказка: тег figcaption). Оформите результат в пандас-таблицу."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "936ac0d1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                          Изображение  \\\n",
            "0   //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "1   //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "2   //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "3   //upload.wikimedia.org/wikipedia/ru/thumb/5/58...   \n",
            "4   //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "5   //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "6   //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "7   //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "8   //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "9   //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "10  //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "11  //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "12  //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "13  //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "14  //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "15  //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "16  //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "17  //upload.wikimedia.org/wikipedia/commons/thumb...   \n",
            "\n",
            "                                              Подпись  \n",
            "0                                             Bitcoin  \n",
            "1                Трезор — аппаратный хранитель ключей  \n",
            "2   Пара ключей и биткойн-адрес для печати на бума...  \n",
            "3   Сравнение традиционной модели приватности с мо...  \n",
            "4   Упрощённая структура последовательных транзакц...  \n",
            "5   Примеры множественных входов и выходов в транз...  \n",
            "6   Основная последовательность блоков (чёрные) яв...  \n",
            "7   Количество биткойнов с течением времени (годы ...  \n",
            "8           Логарифмический график сложности майнинга  \n",
            "9   Блок из нескольких ASIC-плат в форме USB-модул...  \n",
            "10            Количество транзакций биткойнов в месяц  \n",
            "11              Скриншот программы-клиента Bitcoin-qt  \n",
            "12  Металлические биткойн-монеты, выпущенные компа...  \n",
            "13  Цены на золото во время и после золотого станд...  \n",
            "14  В этом кафе можно расплатиться биткойнами. Нид...  \n",
            "15   Линейный график курса Bitcoin по отношению к USD  \n",
            "16  Логарифмический график курса Bitcoin по отноше...  \n",
            "17  Банкомат, поддерживающий операции с биткойнами...  \n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Получение HTML-страницы о Биткойне\n",
        "response = requests.get('https://ru.wikipedia.org/wiki/Биткойн')\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Сбор данных об изображениях\n",
        "images = []\n",
        "captions = []\n",
        "\n",
        "# Поиск всех изображений (тег figure)\n",
        "for figure in soup.find_all('figure'):\n",
        "    img = figure.find('img')\n",
        "    if img and 'src' in img.attrs:\n",
        "        images.append(img['src'])\n",
        "        caption = figure.find('figcaption')\n",
        "        captions.append(caption.get_text() if caption else 'Нет подписи')\n",
        "\n",
        "# Создание DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Изображение': images,\n",
        "    'Подпись': captions\n",
        "})\n",
        "\n",
        "# Вывод таблицы\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f4bdf1-1efd-4ebc-b3ab-bf8f886fa1f7",
      "metadata": {
        "id": "c9f4bdf1-1efd-4ebc-b3ab-bf8f886fa1f7",
        "tags": []
      },
      "source": [
        "## 3. Краулинг форума"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a8b6b37-a45f-41e0-8a03-6f0f25635562",
      "metadata": {
        "id": "1a8b6b37-a45f-41e0-8a03-6f0f25635562",
        "tags": []
      },
      "source": [
        "### Задание\n",
        "Распарсите все страницы из раздела \"Ветеринария КРС\" https://fermer.ru/forum/zhivotnovodstvo/krupnyi-rogatyi-skot/veterinariya-krs. С каждой страницы соберите таблицу и сохраните из нее следующую информацию: имя темы, ссылка на тему, количество оветов, количество просмотров, имя автора темы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e7face1e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Парсинг страницы: https://fermer.ru/forum/zhivotnovodstvo/krupnyi-rogatyi-skot/veterinariya-krs?page=0\n",
            "Парсинг страницы: https://fermer.ru/forum/zhivotnovodstvo/krupnyi-rogatyi-skot/veterinariya-krs?page=1\n",
            "Парсинг страницы: https://fermer.ru/forum/zhivotnovodstvo/krupnyi-rogatyi-skot/veterinariya-krs?page=2\n",
            "Парсинг страницы: https://fermer.ru/forum/zhivotnovodstvo/krupnyi-rogatyi-skot/veterinariya-krs?page=3\n",
            "Парсинг страницы: https://fermer.ru/forum/zhivotnovodstvo/krupnyi-rogatyi-skot/veterinariya-krs?page=4\n",
            "                                                  Тема  \\\n",
            "0    Массовый падёж коров. При вскрытии - страшная ...   \n",
            "1    Годовалый бык сильно худеет. Ноги трясутся, ес...   \n",
            "2      На задней ноге быка шишка. Фото  Опасно ли это?   \n",
            "3    В хозяйстве стали рождаться от нетелей телята ...   \n",
            "4    Обнаружил у коровы в районе правой передней ло...   \n",
            "..                                                 ...   \n",
            "120                         Корова выплёвывает жвачку.   \n",
            "121                                   Вопрос-ответ КРС   \n",
            "122                             Почему горькие сливки?   \n",
            "123                          Доля хромых коров в стаде   \n",
            "124  Одна из коров, как только вводим жом начиет оп...   \n",
            "\n",
            "                                                Ссылка Ответы Просмотры  \\\n",
            "0      https://fermer.ru/forum/419/padezh-korov-382692      7       857   \n",
            "1    https://fermer.ru/forum/567/byk-silno-hudeet-n...      8       343   \n",
            "2    https://fermer.ru/forum/567/na-zadney-noge-byk...      1      1216   \n",
            "3    https://fermer.ru/forum/567/proshu-pomoshchi-u...     18      6186   \n",
            "4    https://fermer.ru/forum/428/kozhnoe-zabolevani...      7       393   \n",
            "..                                                 ...    ...       ...   \n",
            "120  https://fermer.ru/forum/223/korova-vyplyovyvae...      1      4371   \n",
            "121  https://fermer.ru/forum/zhivotnovodstvo/krupny...    119     23751   \n",
            "122   https://fermer.ru/forum/567/gorkie-slivki-359451      3      2670   \n",
            "123  https://fermer.ru/forum/567/dolya-hromyh-korov...      0       654   \n",
            "124  https://fermer.ru/blog/320427/zabolela-korova-...      6      1479   \n",
            "\n",
            "                   Автор                          Последнее обновление  \n",
            "0                 Лина26    от wasia habiboolin пн, 31.03.2025 - 00:11  \n",
            "1               goalhome                 от ПМД сб, 22.03.2025 - 15:46  \n",
            "2                 joxipa  от Кострома предпр... вс, 02.02.2025 - 09:55  \n",
            "3              Георгий63          от ZABAIKALKA вс, 26.01.2025 - 20:53  \n",
            "4                21_Alex             от 21_Alex чт, 16.01.2025 - 15:54  \n",
            "..                   ...                                           ...  \n",
            "120          Трактористт   от Ульяна Никольская пт, 11.12.2020 - 20:47  \n",
            "121           alex113325  от Ольга Вячеславовна пт, 20.11.2020 - 22:40  \n",
            "122  Светлана Силантьева            от ILDARTIR вт, 10.11.2020 - 19:01  \n",
            "123                 Gur2                от Gur2 вт, 03.11.2020 - 11:59  \n",
            "124   Екатерина Канатова              от olga k вс, 01.11.2020 - 08:59  \n",
            "\n",
            "[125 rows x 6 columns]\n",
            "Сохранено 125 тем в fermer_forum_topics.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "base_url = 'https://fermer.ru/forum/zhivotnovodstvo/krupnyi-rogatyi-skot/veterinariya-krs'\n",
        "all_data = []\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "def parse_page(page_url):\n",
        "    try:\n",
        "        response = requests.get(page_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        \n",
        "        topics_table = soup.find('table', class_='forum-table-topics')\n",
        "        \n",
        "        if not topics_table:\n",
        "            print(f\"Не удалось найти таблицу тем на странице {page_url}\")\n",
        "            return\n",
        "        \n",
        "        for row in topics_table.find_all('tr')[1:]:\n",
        "            try:\n",
        "                cols = row.find_all('td')\n",
        "                if len(cols) < 5:\n",
        "                    continue\n",
        "                \n",
        "                # Находим ссылку с названием темы (может быть вложенной)\n",
        "                topic_link_tag = cols[1].find('a', href=True)\n",
        "                topic_name = \"\"\n",
        "                \n",
        "                # Ищем название темы в разных вариантах\n",
        "                title_span = cols[1].find('span', class_='forum-topic-title')\n",
        "                if title_span:\n",
        "                    topic_name = title_span.text.strip()\n",
        "                elif topic_link_tag:\n",
        "                    topic_name = topic_link_tag.text.strip()\n",
        "                \n",
        "                # Если название не найдено, пропускаем тему\n",
        "                if not topic_name:\n",
        "                    continue\n",
        "                \n",
        "                # Получаем ссылку\n",
        "                topic_link = topic_link_tag['href'] if topic_link_tag else \"#\"\n",
        "                \n",
        "                # Автор и дата\n",
        "                author_info = cols[1].find('a', class_='username')\n",
        "                author = author_info.text.strip() if author_info else \"Неизвестно\"\n",
        "                \n",
        "                # Ответы и просмотры\n",
        "                replies = cols[2].text.strip()\n",
        "                views = cols[3].text.strip().replace(',', '')  # Удаляем запятые в числах\n",
        "                \n",
        "                # Последнее обновление\n",
        "                last_update = cols[4].get_text(separator=' ', strip=True)\n",
        "                \n",
        "                all_data.append({\n",
        "                    'Тема': topic_name,\n",
        "                    'Ссылка': f\"https://fermer.ru{topic_link}\" if not topic_link.startswith('http') else topic_link,\n",
        "                    'Ответы': replies,\n",
        "                    'Просмотры': views,\n",
        "                    'Автор': author,\n",
        "                    'Последнее обновление': last_update\n",
        "                })\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при парсинге строки: {e}\")\n",
        "                continue\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при запросе страницы {page_url}: {e}\")\n",
        "\n",
        "# Парсим первые 5 страниц\n",
        "for page_num in range(0, 5):\n",
        "    page_url = f\"{base_url}?page={page_num}\"\n",
        "    print(f\"Парсинг страницы: {page_url}\")\n",
        "    parse_page(page_url)\n",
        "    time.sleep(2)\n",
        "\n",
        "# Создаем DataFrame\n",
        "if all_data:\n",
        "    forum_df = pd.DataFrame(all_data)\n",
        "    print(forum_df)\n",
        "    \n",
        "    # Сохраняем в CSV\n",
        "    forum_df.to_csv('fermer_forum_topics.csv', index=False, encoding='utf-8-sig')\n",
        "    print(f\"Сохранено {len(forum_df)} тем в fermer_forum_topics.csv\")\n",
        "else:\n",
        "    print(\"Не удалось получить данные. Проверьте структуру сайта.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f80199e-bba1-40a0-8aca-0e97c8787b0e",
      "metadata": {
        "id": "4f80199e-bba1-40a0-8aca-0e97c8787b0e",
        "tags": []
      },
      "source": [
        "## 4. Тестовое API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f384004f-8673-4bdf-8cd9-dcc9dc3efdc9",
      "metadata": {
        "id": "f384004f-8673-4bdf-8cd9-dcc9dc3efdc9",
        "tags": []
      },
      "source": [
        "### Задание\n",
        "Получите данные последних 6 пользователей с сайта https://fakestoreapi.com с помощью API.  \n",
        "Подсказка: используйте параметр sort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8b0a7720",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id                                             name  username  \\\n",
            "0  10     {'firstname': 'jimmie', 'lastname': 'klein'}  jimmie_k   \n",
            "1   9        {'firstname': 'kate', 'lastname': 'hale'}    kate_h   \n",
            "2   8  {'firstname': 'william', 'lastname': 'hopkins'}   hopkins   \n",
            "3   7    {'firstname': 'miriam', 'lastname': 'snyder'}    snyder   \n",
            "4   6    {'firstname': 'david', 'lastname': 'russell'}   david_r   \n",
            "5   5     {'firstname': 'derek', 'lastname': 'powell'}     derek   \n",
            "\n",
            "               email  \n",
            "0   jimmie@gmail.com  \n",
            "1     kate@gmail.com  \n",
            "2  william@gmail.com  \n",
            "3   miriam@gmail.com  \n",
            "4  david_r@gmail.com  \n",
            "5    derek@gmail.com  \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Получение данных последних 6 пользователей\n",
        "response = requests.get('https://fakestoreapi.com/users?sort=desc&limit=6')\n",
        "users = response.json()\n",
        "\n",
        "# Создание DataFrame\n",
        "users_df = pd.DataFrame(users)\n",
        "\n",
        "# Вывод данных\n",
        "print(users_df[['id', 'name', 'username', 'email']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eY9iksIEsT2w",
      "metadata": {
        "id": "eY9iksIEsT2w"
      },
      "source": [
        "**Результат:** решение для каждого задания предоставляется в виде программного кода на языке Python\n",
        "\n",
        "**Критерии оценивания:**\n",
        "\n",
        "К1 - отправлен get запрос на сайт из Задания 1, выведен статус-код операции (4 балла)\n",
        "\n",
        "К2 - распарсена страница сайта из Задания 2, собраны  ссылки на изображения и подписи к этим изображениям. Собранные данные представлены в виде пандас-таблицы (6 баллов)\n",
        "\n",
        "К3 - распарсены все страницы из раздела \"Ветеринария КРС\" из Задания 3, с каждой страницы взята таблица и из нее сохранена соответствующая условию задания ифнормация (6 баллов)\n",
        "\n",
        "К4 -  получены данные последних 6 пользователей с сайта из Задания 4 с помощью API (4 балла)\n",
        "\n",
        "\n",
        "Максимальное количество баллов —  20 баллов.\n",
        "\n",
        "Минимальное количество баллов, чтобы преподаватель смог зачесть вашу работу — 10 баллов."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
